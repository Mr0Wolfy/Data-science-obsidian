# Параметры больших данных и формула "V"


Основные характеристики Big Data (больших данных) определяют как шесть «V»:  
  
● **Volume** — объём — от 150 Гб в сутки.  
  
● **Velocity** — скорость. Объём и содержимое Big Data ежесекундно меняются, поэтому собирать и обрабатывать их нужно на больших вычислительных мощностях. Например, сервис [FlightRadar24](https://www.flightradar24.com/), где отображаются все маршруты самолётов в режиме онлайн.  
  
● **Variety** — разнообразие. Массив больших данных может включать фото, видео и тексты, файлы разных объёмов и форматов, данные из множества разных источников. Обычные данные, как правило, однородные, например таблица Excel с Ф. И. О. каждого сотрудника.  
  
● **Veracity** — достоверность. Большие данные собирают только из источников, которым можно доверять, а для анализа используют точные и объективные методы. Поэтому корпорации и международные организации принимают стратегические решения на основе этих данных.  
  
● **Variability** — изменчивость. Большие данные обновляются в режиме онлайн, поэтому их поток нестабилен. На него влияют скорость передачи, изменение источников, действия пользователей и даже смена сезонов. При анализе данных нужно учитывать и прогнозировать все эти факторы. Например, данные об авиаперелётах стоит использовать с поправками на задержки рейсов и погодные условия, из-за которых меняются маршруты.  
  
● **Value** — ценность. Сами по себе данные ничего не значат, но на их основе можно сделать глубокие выводы и принимать взвешенные решения. Например, проанализировать трафик на дорогах в течение года и понять, как лучше построить маршруты для городского транспорта.

Иногда добавляют 7V
● **Visualization** – Визуализация. это необходимая часть анализа, поскольку именно визуализация делает большие данные доступными для человеческого восприятия. Визуализация больших объемов сложных данных гораздо более эффективна и понятна для человека, чем электронные таблицы и отчеты, полные чисел и формул. Конечно, визуализация в рамках Big Data не означает построение обычных графиков или круговых диаграмм: возможно, будут построены сложные графики, которые будут включать в себя множество переменных данных, однако они все равно останутся понятными и читаемыми.

![[Pasted image 20240415105515.png]]




# Как работает технология Big Data: сбор, хранение, обработка

## 1. Сбор

Большие данные собирают из разных источников:  
  
● **Социальные** — всё, что публикуют и делают пользователи в соцсетях, онлайн-сервисах и приложениях. Сюда относят фото, видео, аудио, сообщения в мессенджерах, геолокации и хештеги.  
  
● **Статистические** — все данные от госорганов и исследовательских компаний о людях, животных, транспортных средствах, товарах и услугах, политических и экономических явлениях.  
  
● **Медицинские** — данные из электронных карт о медицинских показаниях, анализах, аппаратной диагностике, вакцинациях, историях болезней.  
  
● **Машинные** — записи с камер наблюдения, видеорегистраторов, систем управления и умных устройств.  
  
● **Транзакционные** — данные о платежах и переводах через банки и другие финансовые сервисы.  
  
В процессе сбора [данные проходят очистку](https://practicum.yandex.ru/blog/chto-takoe-ochistka-dannyh/), или Data Cleaning. На этом этапе, с помощью специальных программ, данные находят, отбирают и фильтруют, проверяя на точность и соответствие заданным параметрам. Специалисты по Data Cleaning размечают массивы данных так, чтобы алгоритмам было проще находить нужные сегменты информации в ответ на запросы пользователей.  
  
Данные извлекают полностью или частично — с момента последнего успешного извлечения. Например, когда составляют прогноз погоды, используют только данные за последние сутки. Чтобы извлечь Big Data за определённый период, в хранилище используют специальную функцию захвата данных.

## 2. Хранение

Обычные данные помещаются на одном компьютере или онлайн-диске. С большими данными так не получится, поэтому их хранят и обрабатывают с помощью облачных серверов и распределённых вычислительных мощностей. Благодаря этому с Big Data одновременно могут работать несколько человек, получая доступ из разных точек.  
  
Для хранения больших данных используют:  
  
● **DWH** — или data warehouse — единое хранилище для всех данных, на основе которых компания формирует отчёты и принимает решения. Файлы в них сгруппированы по областям применения и расположены по хронологии. Например, DWH интернет-магазина, где собраны данные обо всех клиентах, транзакциях и подразделениях. Данные в них поступают по [принципу ETL](https://practicum.yandex.ru/blog/chto-takoe-etl/) (от англ. Extract, Transform, Load): сначала извлекаются, затем трансформируются, а потом загружаются в едином формате.  

● **[[Data Lake]]** — [озёра данных](https://practicum.yandex.ru/blog/chto-takoe-ozera-dannyh/), которые не имеют единого формата и чёткой структуры. Порядок действий здесь такой: извлечение, загрузка в базу и трансформация в формат, который подходит для текущих задач. Озеро данных напоминает виртуальный диск, где хранятся тексты, фото и PDF, а база данных — это таблица, где все они перечислены.  
  
● **СУБД** — [системы управления базами данных](https://practicum.yandex.ru/blog/chto-takoe-subd/), бывают реляционными или нереляционными. Для работы с большими данными чаще используются первые — данные в них организованы в виде таблиц, которые связаны между собой ключами, а для запросов используют [специальный язык — SQL](https://practicum.yandex.ru/blog/chto-takoe-sql/). Например, в колоночной СУБД ClickHouse, которую используют в сервисе Яндекс Метрика, все данные о трафике сайтов хранятся в нескольких таблицах. Строки — это события, например просмотры страниц, а колонки — параметры, например переходы с мобильных устройств. Это позволяет за секунды сформировать отчёт о трафике для сайта по нескольким параметрам сразу, поскольку все они расположены в соседних ячейках.  
  
Многие компании используют нереляционные СУБД. В них данные преобразуются не в связанные друг с другом таблицы, а хранятся по другой, заранее заданной схеме. Это позволяет быстро помещать и извлекать нужную информацию из хранилища, а также запускать высоконагруженные приложения. Допустим, DynamoDB от Amazon — бессерверная СУБД типа NoSQL, которая поддерживает разные языки запросов. Её используют в системах «умного дома», онлайн-играх и рекламных сервисах.

## 3. Обработка

Информацию большого объёма с помощью обычных инструментов обработать будет сложно: на это уйдёт слишком много времени. Для этих задач применяют особое ПО, которое работает по [технологии MapReduce](https://practicum.yandex.ru/blog/gde-i-zachem-ispolzuetsya-hadoop/). Сначала алгоритм отбирает данные по заданным параметрам, затем распределяет между отдельными узлами, серверами или компьютерами, а потом они одновременно обрабатывают эти сегменты данных, параллельно друг с другом.  
  
Вот примеры сервисов, которые используют MapReduce:  
  
● [[Hadoop]] — сервис с открытым исходным кодом, позволяющий собирать, хранить и работать с Big Data сразу нескольким специалистам. Он автоматически перераспределяет нагрузку так, чтобы при отказе одного из узлов другие продолжили работать вместо него.  
● [[Apache Spark]] — сервис из нескольких библиотек для работы с потоковыми данными, которые обновляются с высокой скоростью. При этом данные внутри можно фильтровать, обрабатывать и применять для машинного обучения нейросетей.  
  
Чтобы следить за качеством сбора данных, структурировать их и находить нужное, некоторые компании нанимают специалистов — DWH-аналитиков.

## 4. Анализ

Чтобы применять большие данные в работе, необходимо анализировать их по самым разным параметрам. В этом помогают:  
  
● SQL — язык запросов, который применяют при работе с реляционными СУБД.  
  
● Нейросети, натренированные с помощью машинного обучения так, чтобы за секунды обработать тонны информации и представить точные данные для самых сложных задач.  
  
Чтобы извлекать нужные сегменты информации и преобразовывать их в понятные отчёты и графики, используют специальные аналитические сервисы на базе Business Intelligence (BI). Например, Power BI Microsoft — сервис бизнес-аналитики, который собирает данные из CRM, Excel-таблиц и других источников, а затем представляет их в виде интерактивных отчётов.  
  
На курсе «Специалист по Data Science» студентов учат работать с Big Data с помощью самых популярных инструментов и технологий. А главное — извлекать из «сырых» данных самое ценное и принимать решения, которые помогут бизнесу.